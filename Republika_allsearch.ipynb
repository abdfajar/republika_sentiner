{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs2vu9sq4yWfv5anHnQ7H+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdfajar/republika_sentiner/blob/main/Republika_allsearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "o7QYVOKMN05i",
        "outputId": "9216b4fe-3298-4c79-d623-e7b171894704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Republika Search Scraper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-587377960.py:167: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-587377960.py:167: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import gradio as gr\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, quote_plus\n",
        "import hashlib\n",
        "\n",
        "def generate_search_id(keyword, startdate, enddate, page):\n",
        "    \"\"\"Generate unique search_id based on inputs\"\"\"\n",
        "    input_str = f\"{keyword}_{startdate}_{enddate}_{page}\"\n",
        "    return hashlib.md5(input_str.encode()).hexdigest()[:16]\n",
        "\n",
        "def scrape_republika_search(keyword, startdate, enddate, page=1):\n",
        "    \"\"\"\n",
        "    Scrape search results from Republika.co.id\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Build search URL\n",
        "        q = quote_plus(keyword)\n",
        "        url = f\"https://republika.co.id/search/v3/all/{page}/?q={q}&latest_date=custom&startdate={startdate}&enddate={enddate}\"\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        }\n",
        "\n",
        "        print(f\"üîç Scraping: {url}\")\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Target selector based on user description\n",
        "        selector = \"#search > div.main-wrapper > main > div.main-content > div.container > div.results-section\"\n",
        "        results_section = soup.select_one(selector)\n",
        "\n",
        "        if not results_section:\n",
        "            # Fallback selectors\n",
        "            fallback_selectors = [\n",
        "                'div.results-section',\n",
        "                '.results-section',\n",
        "                'main div.container div[class*=\"result\"]',\n",
        "                '.search-results'\n",
        "            ]\n",
        "            for sel in fallback_selectors:\n",
        "                results_section = soup.select_one(sel)\n",
        "                if results_section:\n",
        "                    print(f\"‚úÖ Found results with fallback: {sel}\")\n",
        "                    break\n",
        "\n",
        "        if not results_section:\n",
        "            return [], \"‚ùå Results section not found. Structure may have changed.\"\n",
        "\n",
        "        # Extract individual results - assume common patterns in Republika\n",
        "        # Look for article cards: div with h2/h3 + date + a href\n",
        "        results_list = []\n",
        "\n",
        "        # Possible item selectors (adapt based on common Republika structure)\n",
        "        item_selectors = [\n",
        "            'div[class*=\"card\"] a',\n",
        "            'article a',\n",
        "            '.search-item',\n",
        "            '.result-item',\n",
        "            'div.max-card'\n",
        "        ]\n",
        "\n",
        "        items = []\n",
        "        for sel in item_selectors:\n",
        "            items = results_section.select(sel)\n",
        "            if items:\n",
        "                print(f\"‚úÖ Found {len(items)} items with selector: {sel}\")\n",
        "                break\n",
        "        else:\n",
        "            # Ultimate fallback: find all a with href containing /berita/ or similar\n",
        "            items = results_section.find_all('a', href=re.compile(r'/berita/|/reads/'))\n",
        "\n",
        "        for item in items[:20]:  # Limit to top 20 per page\n",
        "            # Extract title (h1-h4 or text in a)\n",
        "            title_elem = item.find(['h1', 'h2', 'h3', 'h4']) or item\n",
        "            title = title_elem.get_text(strip=True)\n",
        "            if not title or len(title) < 10:\n",
        "                continue\n",
        "\n",
        "            # Extract date (look for patterns like \"DD Month YYYY, HH:MM\")\n",
        "            date_elem = item.find(class_=re.compile(r'date|time')) or item.find('span')\n",
        "            date_text = date_elem.get_text(strip=True) if date_elem else \"\"\n",
        "            date_match = re.search(r'(\\d{1,2}\\s+\\w+\\s+\\d{4},\\s+\\d{1,2}:\\d{2})', date_text)\n",
        "            date = date_match.group(1) if date_match else \"Date not found\"\n",
        "\n",
        "            # Extract URL\n",
        "            href = item.get('href', '')\n",
        "            if href.startswith('/'):\n",
        "                full_url = urljoin(\"https://republika.co.id\", href)\n",
        "            else:\n",
        "                full_url = href\n",
        "\n",
        "            results_list.append({\n",
        "                'title': title[:200],  # Truncate long titles\n",
        "                'date': date,\n",
        "                'url': full_url\n",
        "            })\n",
        "\n",
        "        status = f\"‚úÖ Found {len(results_list)} results on page {page}\"\n",
        "        return results_list, status\n",
        "\n",
        "    except Exception as e:\n",
        "        return [], f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "def process_republika_search(keyword, startdate, enddate, page):\n",
        "    \"\"\"\n",
        "    Main processing function for Gradio\n",
        "    \"\"\"\n",
        "    if not keyword.strip():\n",
        "        return \"‚ùå Masukkan keyword pencarian!\", pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # Format dates if provided\n",
        "    if startdate:\n",
        "        startdate_str = startdate.strftime('%Y-%m-%d')\n",
        "    else:\n",
        "        startdate_str = '2025-10-01'\n",
        "\n",
        "    if enddate:\n",
        "        enddate_str = enddate.strftime('%Y-%m-%d')\n",
        "    else:\n",
        "        enddate_str = '2025-10-31'\n",
        "\n",
        "    results_list, status = scrape_republika_search(keyword, startdate_str, enddate_str, page)\n",
        "\n",
        "    if not results_list:\n",
        "        return status, pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # Create KeywordSearchResult schema\n",
        "    search_id = generate_search_id(keyword, startdate_str, enddate_str, page)\n",
        "    timestamp_search = datetime.now().isoformat()\n",
        "    num_results = len(results_list)\n",
        "    results_json = json.dumps(results_list, ensure_ascii=False)\n",
        "\n",
        "    df_keyword_search = pd.DataFrame([{\n",
        "        'search_id': search_id,\n",
        "        'keyword': keyword,\n",
        "        'source_type': 'Republika Search',\n",
        "        'num_results': num_results,\n",
        "        'results': results_json[:1000] + \"...\" if len(results_json) > 1000 else results_json,  # Truncate for display\n",
        "        'timestamp_search': timestamp_search\n",
        "    }])\n",
        "\n",
        "    # Results table\n",
        "    df_results = pd.DataFrame(results_list)\n",
        "\n",
        "    output_msg = f\"\"\"üîç **HASIL PENCARIAN REPUBLIKA.CO.ID**\n",
        "üìù **Keyword:** {keyword}\n",
        "üìÖ **Periode:** {startdate_str} s.d. {enddate_str}\n",
        "üìÑ **Halaman:** {page}\n",
        "{status}\n",
        "\n",
        "**üíæ Search ID:** `{search_id}`\n",
        "\n",
        "**üì§ Export CSV:**\n",
        "- Klik tombol di bawah untuk download `keyword_search.csv` dan `search_results.csv`\"\"\"\n",
        "\n",
        "    return output_msg, df_results, df_keyword_search\n",
        "\n",
        "# Gradio Interface\n",
        "def create_search_interface():\n",
        "    with gr.Blocks(\n",
        "        title=\"üîç Republika.co.id Search Scraper - KeywordSearchResult\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .search-box { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }\n",
        "        \"\"\"\n",
        "    ) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üîç **Republika.co.id Search Scraper**\n",
        "        **Ekstrak hasil pencarian berdasarkan keyword ke dalam skema KeywordSearchResult**\n",
        "\n",
        "        **Fitur:**\n",
        "        - ‚úÖ Scraping multi-halaman\n",
        "        - ‚úÖ Ekstraksi otomatis title, date, URL\n",
        "        - ‚úÖ Generate Search ID unik\n",
        "        - ‚úÖ Export ke CSV sesuai skema\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                keyword_input = gr.Textbox(\n",
        "                    label=\"üîë Keyword Pencarian\",\n",
        "                    placeholder=\"e.g., MBG\",\n",
        "                    value=\"MBG\"\n",
        "                )\n",
        "                startdate_input = gr.Textbox(\n",
        "                    label=\"üìÖ Tanggal Mulai (YYYY-MM-DD)\",\n",
        "                    placeholder=\"2025-10-01\",\n",
        "                    value=\"2025-10-01\"\n",
        "                )\n",
        "                enddate_input = gr.Textbox(\n",
        "                    label=\"üìÖ Tanggal Selesai (YYYY-MM-DD)\",\n",
        "                    placeholder=\"2025-10-31\",\n",
        "                    value=\"2025-10-31\"\n",
        "                )\n",
        "                page_input = gr.Number(\n",
        "                    label=\"üìÑ Nomor Halaman\",\n",
        "                    value=1,\n",
        "                    minimum=1,\n",
        "                    maximum=50\n",
        "                )\n",
        "                search_btn = gr.Button(\"üöÄ Cari & Scrap\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Row():\n",
        "            output_msg = gr.Markdown(label=\"üìä Status & Info\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üìã Tabel Hasil Pencarian\")\n",
        "                results_table = gr.Dataframe(\n",
        "                    label=\"Results\",\n",
        "                    headers=[\"title\", \"date\", \"url\"],\n",
        "                    wrap=True,\n",
        "                    interactive=False\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üíæ Skema KeywordSearchResult\")\n",
        "                schema_table = gr.Dataframe(\n",
        "                    label=\"Keyword Search Data\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        # Export buttons\n",
        "        with gr.Row():\n",
        "            export_search = gr.File(\n",
        "                label=\"üíæ Download keyword_search.csv\",\n",
        "                visible=False\n",
        "            )\n",
        "            export_results = gr.File(\n",
        "                label=\"üíæ Download search_results.csv\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "        # Events\n",
        "        def parse_dates(startdate_str, enddate_str):\n",
        "            try:\n",
        "                startdate = datetime.strptime(startdate_str, '%Y-%m-%d') if startdate_str else None\n",
        "                enddate = datetime.strptime(enddate_str, '%Y-%m-%d') if enddate_str else None\n",
        "                return startdate, enddate\n",
        "            except ValueError:\n",
        "                raise gr.Error(\"Format tanggal harus YYYY-MM-DD!\")\n",
        "\n",
        "        def export_csv(df_keyword, df_results):\n",
        "            search_csv = df_keyword.to_csv(index=False)\n",
        "            results_csv = df_results.to_csv(index=False)\n",
        "            return search_csv.encode(), results_csv.encode()\n",
        "\n",
        "        search_btn.click(\n",
        "            fn=lambda k, s, e, p: process_republika_search(\n",
        "                k,\n",
        "                parse_dates(s, e)[0],\n",
        "                parse_dates(s, e)[1],\n",
        "                p\n",
        "            ) if s and e else process_republika_search(k, None, None, p),\n",
        "            inputs=[keyword_input, startdate_input, enddate_input, page_input],\n",
        "            outputs=[output_msg, results_table, schema_table]\n",
        "        ).then(\n",
        "            fn=export_csv,\n",
        "            inputs=[schema_table, results_table],\n",
        "            outputs=[export_search, export_results]\n",
        "        )\n",
        "\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"MBG\", \"2025-10-01\", \"2025-10-31\", 1],\n",
        "                [\"Prabowo\", \"2025-09-01\", \"2025-09-30\", 2],\n",
        "            ],\n",
        "            inputs=[keyword_input, startdate_input, enddate_input, page_input]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting Republika Search Scraper...\")\n",
        "    demo = create_search_interface()\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7861,\n",
        "        share=False,\n",
        "        inbrowser=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import gradio as gr\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, quote_plus\n",
        "import hashlib\n",
        "import time\n",
        "import os\n",
        "\n",
        "# ==================== FUNGSI SCRAPING ARTIKEL ====================\n",
        "def clean_text(text):\n",
        "    \"\"\"Membersihkan teks dari karakter tidak diinginkan\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:()\\-]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_element(element):\n",
        "    \"\"\"Ekstrak teks dari elemen dengan pembersihan\"\"\"\n",
        "    if not element:\n",
        "        return \"\"\n",
        "    element_copy = BeautifulSoup(str(element), 'html.parser')\n",
        "    for unwanted in element_copy(['script', 'style', 'nav', 'header', 'footer', 'aside', 'figure', 'img', 'video', 'blockquote']):\n",
        "        unwanted.decompose()\n",
        "    text = element_copy.get_text(separator='\\n', strip=True)\n",
        "    return clean_text(text)\n",
        "\n",
        "def extract_republika_article(url):\n",
        "    \"\"\"\n",
        "    Fungsi utama untuk scraping artikel Republika.co.id\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        }\n",
        "        print(f\"üîç Mengakses URL: {url}\")\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        metadata = {\n",
        "            'judul': '',\n",
        "            'waktu_terbit': '',\n",
        "            'editor': '',\n",
        "            'konten': '',\n",
        "            'url': url,\n",
        "            'panjang_konten': 0\n",
        "        }\n",
        "        main_content = soup.find('div', class_='main-content__left')\n",
        "        if not main_content:\n",
        "            return None, \"Struktur halaman tidak dikenali. Tidak ditemukan div.main-content__left\"\n",
        "        title_div = main_content.find('div', class_='max-card__title')\n",
        "        if title_div:\n",
        "            title_h1 = title_div.find('h1')\n",
        "            metadata['judul'] = clean_text(title_h1.get_text()) if title_h1 else \"Judul tidak ditemukan\"\n",
        "        else:\n",
        "            title_h1 = main_content.find('h1')\n",
        "            metadata['judul'] = clean_text(title_h1.get_text()) if title_h1 else \"Judul tidak ditemukan\"\n",
        "        date_element = main_content.find('div', class_='date date-item__headline')\n",
        "        if date_element:\n",
        "            date_text = clean_text(date_element.get_text())\n",
        "            date_patterns = [\n",
        "                r'(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s+(\\d{1,2}:\\d{2})',\n",
        "                r'(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,2}:\\d{2})',\n",
        "                r'(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n",
        "            ]\n",
        "            for pattern in date_patterns:\n",
        "                match = re.search(pattern, date_text)\n",
        "                if match:\n",
        "                    if len(match.groups()) == 2:\n",
        "                        metadata['waktu_terbit'] = f\"{match.group(1)} {match.group(2)} WIB\"\n",
        "                    else:\n",
        "                        metadata['waktu_terbit'] = f\"{match.group(1)}\"\n",
        "                    break\n",
        "            else:\n",
        "                metadata['waktu_terbit'] = date_text\n",
        "        else:\n",
        "            metadata['waktu_terbit'] = \"Waktu tidak ditemukan\"\n",
        "        editor_div = main_content.find('div', class_=lambda x: x == '' or x is None)\n",
        "        if editor_div:\n",
        "            editor_text = clean_text(editor_div.get_text())\n",
        "            editor_patterns = [\n",
        "                r'Red\\s*:\\s*([^<]+)',\n",
        "                r'Editor\\s*:\\s*([^<]+)',\n",
        "                r'Reporter\\s*:\\s*([^<]+)'\n",
        "            ]\n",
        "            for pattern in editor_patterns:\n",
        "                match = re.search(pattern, editor_text)\n",
        "                if match:\n",
        "                    metadata['editor'] = clean_text(match.group(1))\n",
        "                    break\n",
        "            if not metadata['editor']:\n",
        "                editor_link = editor_div.find('a')\n",
        "                if editor_link:\n",
        "                    metadata['editor'] = clean_text(editor_link.get_text())\n",
        "        if not metadata['editor']:\n",
        "            all_text = main_content.get_text()\n",
        "            editor_match = re.search(r'Red\\s*:\\s*([^\\n<]+)', all_text)\n",
        "            if editor_match:\n",
        "                metadata['editor'] = clean_text(editor_match.group(1))\n",
        "            else:\n",
        "                metadata['editor'] = \"Editor tidak ditemukan\"\n",
        "        article_content = main_content.find('div', class_='article-content')\n",
        "        if article_content:\n",
        "            konten_artikel = extract_text_from_element(article_content)\n",
        "        else:\n",
        "            fallback_selectors = [\n",
        "                '.article-content',\n",
        "                '.article-body',\n",
        "                '.content',\n",
        "                '.post-content',\n",
        "                '[itemprop=\"articleBody\"]',\n",
        "                '.detail-text'\n",
        "            ]\n",
        "            konten_artikel = \"\"\n",
        "            for selector in fallback_selectors:\n",
        "                content_elem = main_content.select_one(selector)\n",
        "                if content_elem:\n",
        "                    konten_artikel = extract_text_from_element(content_elem)\n",
        "                    break\n",
        "            if not konten_artikel:\n",
        "                konten_artikel = extract_text_from_element(main_content)\n",
        "        metadata['konten'] = konten_artikel\n",
        "        metadata['panjang_konten'] = len(konten_artikel)\n",
        "        return metadata, None\n",
        "    except Exception as e:\n",
        "        return None, f\"Error: {str(e)}\"\n",
        "\n",
        "# ==================== FUNGSI SCRAPING PENCARIAN ====================\n",
        "def generate_search_id(keyword, startdate, enddate):\n",
        "    \"\"\"Generate unique search_id based on inputs (without page)\"\"\"\n",
        "    input_str = f\"{keyword}_{startdate}_{enddate}\"\n",
        "    return hashlib.md5(input_str.encode()).hexdigest()[:16]\n",
        "\n",
        "def scrape_republika_search(keyword, startdate, enddate):\n",
        "    \"\"\"\n",
        "    Scrape all pages from Republika.co.id search until no more results\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "    page = 1\n",
        "    max_pages = 50  # Safety limit to prevent infinite loop\n",
        "    status_msgs = []\n",
        "\n",
        "    while page <= max_pages:\n",
        "        try:\n",
        "            q = quote_plus(keyword)\n",
        "            url = f\"https://republika.co.id/search/v3/all/{page}/?q={q}&latest_date=custom&startdate={startdate}&enddate={enddate}\"\n",
        "\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            }\n",
        "\n",
        "            print(f\"üîç Scraping page {page}: {url}\")\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            selector = \"#search > div.main-wrapper > main > div.main-content > div.container > div.results-section\"\n",
        "            results_section = soup.select_one(selector)\n",
        "\n",
        "            if not results_section:\n",
        "                fallback_selectors = [\n",
        "                    'div.results-section',\n",
        "                    '.results-section',\n",
        "                    'main div.container div[class*=\"result\"]',\n",
        "                    '.search-results'\n",
        "                ]\n",
        "                for sel in fallback_selectors:\n",
        "                    results_section = soup.select_one(sel)\n",
        "                    if results_section:\n",
        "                        print(f\"‚úÖ Found results with fallback: {sel}\")\n",
        "                        break\n",
        "\n",
        "            if not results_section:\n",
        "                status_msgs.append(f\"‚ùå Results section not found on page {page}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            items = []\n",
        "            item_selectors = [\n",
        "                'div[class*=\"card\"] a',\n",
        "                'article a',\n",
        "                '.search-item',\n",
        "                '.result-item',\n",
        "                'div.max-card'\n",
        "            ]\n",
        "\n",
        "            for sel in item_selectors:\n",
        "                items = results_section.select(sel)\n",
        "                if items:\n",
        "                    print(f\"‚úÖ Found {len(items)} items on page {page} with selector: {sel}\")\n",
        "                    break\n",
        "            else:\n",
        "                items = results_section.find_all('a', href=re.compile(r'/berita/|/reads/'))\n",
        "\n",
        "            if not items:\n",
        "                status_msgs.append(f\"‚úÖ No more results on page {page}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            page_results = []\n",
        "            for item in items:\n",
        "                title_elem = item.find(['h1', 'h2', 'h3', 'h4']) or item\n",
        "                title = title_elem.get_text(strip=True)\n",
        "                if not title or len(title) < 10:\n",
        "                    continue\n",
        "\n",
        "                date_elem = item.find(class_=re.compile(r'date|time')) or item.find('span')\n",
        "                date_text = date_elem.get_text(strip=True) if date_elem else \"\"\n",
        "                date_match = re.search(r'(\\d{1,2}\\s+\\w+\\s+\\d{4},\\s+\\d{1,2}:\\d{2})', date_text)\n",
        "                date = date_match.group(1) if date_match else \"Date not found\"\n",
        "\n",
        "                href = item.get('href', '')\n",
        "                if href.startswith('/'):\n",
        "                    full_url = urljoin(\"https://republika.co.id\", href)\n",
        "                else:\n",
        "                    full_url = href\n",
        "\n",
        "                page_results.append({\n",
        "                    'title': title[:200],\n",
        "                    'date': date,\n",
        "                    'url': full_url\n",
        "                })\n",
        "\n",
        "            all_results.extend(page_results)\n",
        "            status_msgs.append(f\"‚úÖ Found {len(page_results)} results on page {page}\")\n",
        "\n",
        "            # Check for next page (look for pagination)\n",
        "            next_page = soup.find('a', class_='next') or soup.find('a', text=re.compile(r'Next|Selanjutnya'))\n",
        "            if not next_page:\n",
        "                status_msgs.append(\"‚úÖ No next page found. Stopping.\")\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "            time.sleep(2)  # Delay to avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "            status_msgs.append(f\"‚ùå Error on page {page}: {str(e)}. Stopping.\")\n",
        "            break\n",
        "\n",
        "    return all_results, \"\\n\".join(status_msgs)\n",
        "\n",
        "# ==================== PROSES UTAMA ====================\n",
        "def process_republika_search(keyword, startdate_str, enddate_str):\n",
        "    if not keyword.strip():\n",
        "        return \"‚ùå Masukkan keyword pencarian!\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    startdate = startdate_str or '2025-10-01'\n",
        "    enddate = enddate_str or '2025-10-31'\n",
        "\n",
        "    results_list, status = scrape_republika_search(keyword, startdate, enddate)\n",
        "\n",
        "    if not results_list:\n",
        "        return status, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    search_id = generate_search_id(keyword, startdate, enddate)\n",
        "    timestamp_search = datetime.now().isoformat()\n",
        "    num_results = len(results_list)\n",
        "    results_json = json.dumps(results_list, ensure_ascii=False)\n",
        "\n",
        "    df_keyword_search = pd.DataFrame([{\n",
        "        'search_id': search_id,\n",
        "        'keyword': keyword,\n",
        "        'source_type': 'Republika Search',\n",
        "        'num_results': num_results,\n",
        "        'results': results_json,\n",
        "        'timestamp_search': timestamp_search\n",
        "    }])\n",
        "\n",
        "    df_results = pd.DataFrame(results_list)\n",
        "\n",
        "    # Scraping metadata artikel untuk setiap URL\n",
        "    articles_metadata = []\n",
        "    for result in results_list:\n",
        "        url = result['url']\n",
        "        metadata, error = extract_republika_article(url)\n",
        "        if metadata:\n",
        "            metadata['search_id'] = search_id\n",
        "            metadata['article_id'] = hashlib.md5(url.encode()).hexdigest()[:16]\n",
        "            metadata['timestamp_ekstraksi'] = datetime.now().isoformat()\n",
        "            articles_metadata.append(metadata)\n",
        "            time.sleep(2)  # Delay\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Failed to scrape {url}: {error}\")\n",
        "\n",
        "    df_metadata = pd.DataFrame(articles_metadata)\n",
        "\n",
        "    # Simpan ke CSV\n",
        "    csv_dir = \"scraping_results\"\n",
        "    os.makedirs(csv_dir, exist_ok=True)\n",
        "\n",
        "    keyword_csv_path = os.path.join(csv_dir, f\"keyword_search_{search_id}.csv\")\n",
        "    results_csv_path = os.path.join(csv_dir, f\"search_results_{search_id}.csv\")\n",
        "    metadata_csv_path = os.path.join(csv_dir, f\"article_metadata_{search_id}.csv\")\n",
        "\n",
        "    df_keyword_search.to_csv(keyword_csv_path, index=False)\n",
        "    df_results.to_csv(results_csv_path, index=False)\n",
        "    df_metadata.to_csv(metadata_csv_path, index=False)\n",
        "\n",
        "    output_msg = f\"\"\"üîç **HASIL PENCARIAN REPUBLIKA.CO.ID**\n",
        "üìù **Keyword:** {keyword}\n",
        "üìÖ **Periode:** {startdate} s.d. {enddate}\n",
        "{status}\n",
        "\n",
        "**üíæ Search ID:** `{search_id}`\n",
        "**üìö Total Hasil:** {num_results}\n",
        "**üìë Metadata Artikel Diekstrak:** {len(articles_metadata)}\n",
        "\n",
        "**üì§ File CSV Tersimpan:**\n",
        "- {keyword_csv_path}\n",
        "- {results_csv_path}\n",
        "- {metadata_csv_path}\n",
        "\n",
        "**üì§ Export CSV di Interface:**\n",
        "- Klik tombol untuk download\"\"\"\n",
        "\n",
        "    return output_msg, df_results, df_keyword_search, df_metadata\n",
        "\n",
        "# ==================== GRADIO INTERFACE ====================\n",
        "def create_search_interface():\n",
        "    with gr.Blocks(\n",
        "        title=\"üîç Republika.co.id Search Scraper - KeywordSearchResult\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .search-box { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }\n",
        "        \"\"\"\n",
        "    ) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üîç **Republika.co.id Search Scraper**\n",
        "        **Ekstrak hasil pencarian berdasarkan keyword ke dalam skema KeywordSearchResult & ArticleMetadata**\n",
        "\n",
        "        **Fitur:**\n",
        "        - ‚úÖ Scraping semua halaman otomatis\n",
        "        - ‚úÖ Ekstraksi otomatis title, date, URL\n",
        "        - ‚úÖ Scraping metadata artikel untuk setiap hasil\n",
        "        - ‚úÖ Generate Search ID unik\n",
        "        - ‚úÖ Simpan ke CSV otomatis\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                keyword_input = gr.Textbox(\n",
        "                    label=\"üîë Keyword Pencarian\",\n",
        "                    placeholder=\"e.g., MBG\",\n",
        "                    value=\"MBG\"\n",
        "                )\n",
        "                startdate_input = gr.Textbox(\n",
        "                    label=\"üìÖ Tanggal Mulai (YYYY-MM-DD)\",\n",
        "                    placeholder=\"2025-10-01\",\n",
        "                    value=\"2025-10-01\"\n",
        "                )\n",
        "                enddate_input = gr.Textbox(\n",
        "                    label=\"üìÖ Tanggal Selesai (YYYY-MM-DD)\",\n",
        "                    placeholder=\"2025-10-31\",\n",
        "                    value=\"2025-10-31\"\n",
        "                )\n",
        "                search_btn = gr.Button(\"üöÄ Cari & Scrap Semua Halaman\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Row():\n",
        "            output_msg = gr.Markdown(label=\"üìä Status & Info\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üìã Tabel Hasil Pencarian\")\n",
        "                results_table = gr.Dataframe(\n",
        "                    label=\"Results\",\n",
        "                    headers=[\"title\", \"date\", \"url\"],\n",
        "                    wrap=True,\n",
        "                    interactive=False\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üíæ Skema KeywordSearchResult\")\n",
        "                schema_table = gr.Dataframe(\n",
        "                    label=\"Keyword Search Data\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            gr.Markdown(\"### üìö Tabel Metadata Artikel\")\n",
        "            metadata_table = gr.Dataframe(\n",
        "                label=\"Article Metadata\",\n",
        "                wrap=True,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        # Export buttons\n",
        "        with gr.Row():\n",
        "            export_search = gr.File(\n",
        "                label=\"üíæ Download keyword_search.csv\",\n",
        "                visible=False\n",
        "            )\n",
        "            export_results = gr.File(\n",
        "                label=\"üíæ Download search_results.csv\",\n",
        "                visible=False\n",
        "            )\n",
        "            export_metadata = gr.File(\n",
        "                label=\"üíæ Download article_metadata.csv\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "        # Events\n",
        "        def parse_dates(startdate_str, enddate_str):\n",
        "            try:\n",
        "                if startdate_str:\n",
        "                    datetime.strptime(startdate_str, '%Y-%m-%d')\n",
        "                if enddate_str:\n",
        "                    datetime.strptime(enddate_str, '%Y-%m-%d')\n",
        "                return startdate_str, enddate_str\n",
        "            except ValueError:\n",
        "                raise gr.Error(\"Format tanggal harus YYYY-MM-DD!\")\n",
        "\n",
        "        def export_csv(df_keyword, df_results, df_metadata):\n",
        "            search_csv = df_keyword.to_csv(index=False).encode()\n",
        "            results_csv = df_results.to_csv(index=False).encode()\n",
        "            metadata_csv = df_metadata.to_csv(index=False).encode()\n",
        "            return search_csv, results_csv, metadata_csv\n",
        "\n",
        "        search_btn.click(\n",
        "            fn=lambda k, s, e: process_republika_search(\n",
        "                k,\n",
        "                parse_dates(s, e)[0],\n",
        "                parse_dates(s, e)[1]\n",
        "            ),\n",
        "            inputs=[keyword_input, startdate_input, enddate_input],\n",
        "            outputs=[output_msg, results_table, schema_table, metadata_table]\n",
        "        ).then(\n",
        "            fn=export_csv,\n",
        "            inputs=[schema_table, results_table, metadata_table],\n",
        "            outputs=[export_search, export_results, export_metadata]\n",
        "        )\n",
        "\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"MBG\", \"2025-10-01\", \"2025-10-31\"],\n",
        "                [\"Prabowo\", \"2025-09-01\", \"2025-09-30\"],\n",
        "            ],\n",
        "            inputs=[keyword_input, startdate_input, enddate_input]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting Republika Search Scraper...\")\n",
        "    demo = create_search_interface()\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "COmKjDkrQO0z",
        "outputId": "cb19ecb2-5e16-40d3-bbd3-4674b49282e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Republika Search Scraper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1021437175.py:323: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-1021437175.py:323: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://585a4ca0815013465c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://585a4ca0815013465c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}